# History leading to SOTA LLM

Timeline from 1990-2023 ...with key breakthroughs, papers, and popular applications

### Statistical Language Models (SLMs) Era: 1990-2010 (with Summaries)

*SLMs analyzed natural language using probabilistic methods, calculating sentence probabilities as products of conditional probabilities.*

- 1990: [Hidden Markov Models for speech recognition](SLM_Hidden-Markov-Models-for-Speech-Recognition.md) (Rabiner) [Voice Command Systems]    
- 1993: [IBM Model 1 for statistical machine translation](SLM_Mathematics-of-Statistical-Machine-Translation.md) (Brown et al.) [Early Online Translation]    
- 1995: [Improved backing-off for M-gram language modeling](SLM_IMPROVED-BACKING-OFF-FOR-M-GRAM-LANGUAGE-MODELING.md) (Kneser & Ney) [Spell Checkers]    
- 1996: [Maximum Entropy Models](SLM_Maximum-Entropy-Approach-NLP.md) (Berger et al.) [Text Classification]    
- 1999: [An empirical study of smoothing techniques for language modeling](SLM_empirical-study-of-smoothing-techniques-for-language-modeling.md) (Chen & Goodman) [Improved Language Models]    
- 2002: [Latent Dirichlet Allocation (LDA)](SLM_Latent-Dirichlet-Allocation.md) (Blei et al.) [Document Clustering]    
- 2006: [Hierarchical Pitman-Yor language model](SLM_A-Hierarchical-Bayesian-Language-Model-based-on-Pitman-Yor-Processes.md) (Teh) [Text Generation]

### Neural Language Models (NLMs) Era: 2011-2017  (future summaries)

*NLMs leveraged neural networks to predict word sequences, introducing word vectors and overcoming SLM limitations.*

- 2012: [AlexNet wins ImageNet competition](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) [Image Recognition]    
- 2013: [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239) (Tang) [Computer Vision]    
- 2013: [Word2Vec introduces efficient word embeddings](https://arxiv.org/pdf/1301.3781) [Search Engines]    
- 2013: [Sequence-to-sequence models emerge](https://arxiv.org/abs/1409.3215) [Machine Translation]    
- 2014: [Attention mechanism introduced](https://arxiv.org/abs/1409.0473) [Neural Machine Translation]    
- 2015: [ResNet surpasses human-level performance on ImageNet](https://ieeexplore.ieee.org/document/7780459) [Image Classification]
    

### Pre-trained Language Models (PLMs) Era: 2018-2020  (future summaries)

*PLMs introduced the "pre-training and fine-tuning" paradigm, training on large volumes of text before task-specific fine-tuning.*

- 2017: [Attention is All You Need](https://arxiv.org/abs/1706.03762) [Language Translation]    
- 2018: [ULMFiT](https://paperswithcode.com/method/ulmfit) (Universal Language Model Fine-tuning) [Text classification]    
- 2018: [ELMo](https://paperswithcode.com/method/elmo) (Embeddings from Language Models) [Named Entity Recognition]    
- 2018: [BERT](https://aclanthology.org/N19-1423.pdf) (Bidirectional Encoder Representations from Transformers) [Question answering]    
- 2019: [GPT-2](https://openai.com/index/better-language-models/) [Text completion and generation]    
- 2019: [XLNet](https://www.semanticscholar.org/paper/XLNet-Transfer-Learning-Model-for-Sentimental-Dhivyaa-Nithya/4b402bc52446f018f3fe7a859d0cd03027c91e5a) [Sentiment analysis]    
- 2019: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) [Natural language inference]    
- 2020: [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB) [Token classification tasks]
    
### Large Language Models (LLMs) Era: 2020-Present  (future summaries)

*LLMs are trained on massive text corpora with billions of parameters, approximating human-level performance in various tasks.*

- 2020: [GPT-3](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) [OpenAI, 175B] [Few-shot learning across various NLP tasks]    
- 2020: [GShard](https://arxiv.org/abs/2006.16668) [Google, 600B] [Multilingual translation]    
- 2021: [Switch Transformer](https://arxiv.org/abs/2101.03961) [Google, 1.6T] [Efficient language modeling]    
- 2021: [Megatron-Turing NLG](https://arxiv.org/abs/2201.11990) [Microsoft & NVIDIA, 530B] [Natural language generation]    
- 2022: [PaLM](https://research.google/blog/pathways-language-model-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/) [Google, 540B] [Reasoning and problem-solving]    
- 2022: [BLOOM](https://arxiv.org/abs/2211.05100) [BigScience, 176B] [Open-source multilingual language model]    
- 2023: [GPT-4](https://arxiv.org/abs/2303.08774) [OpenAI, undisclosed] [Advanced language understanding and generation]
