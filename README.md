# AI Summaries

## About this Repository

its just summaries... 

and here, you can have this timeline too:

## Hisotry leading to SOTA LLM

Timeline from 1990-2023 ...with key breakthroughs, papers, and popular applications

### Statistical Language Models (SLMs) Era: 1990-2010

*SLMs analyzed natural language using probabilistic methods, calculating sentence probabilities as products of conditional probabilities.*

- 1990: [Hidden Markov Models for speech recognition](http://luthuli.cs.uiuc.edu/~daf/courses/Signals%20AI/Papers/HMMs/0.pdf) (Rabiner) [Voice Command Systems]    
- 1993: [IBM Model 1 for statistical machine translation](https://aclanthology.org/J93-2003.pdf) (Brown et al.) [Early Online Translation]    
- 1995: [Improved backing-off for M-gram language modeling](https://ieeexplore.ieee.org/document/479394) (Kneser & Ney) [Spell Checkers]    
- 1996: [Maximum Entropy Models](https://aclanthology.org/J96-1002.pdf) (Berger et al.) [Text Classification]    
- 1999: [An empirical study of smoothing techniques for language modeling](http://www2.denizyuret.com/ref/goodman/chen-goodman-99.pdf) (Chen & Goodman) [Improved Language Models]    
- 2002: [Latent Dirichlet Allocation (LDA)](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) (Blei et al.) [Document Clustering]    
- 2006: [Hierarchical Pitman-Yor language model](https://www.stats.ox.ac.uk/~teh/research/compling/acl2006.pdf) (Teh) [Text Generation]
    

### Neural Language Models (NLMs) Era: 2011-2017

*NLMs leveraged neural networks to predict word sequences, introducing word vectors and overcoming SLM limitations.*

- 2012: [AlexNet wins ImageNet competition](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) [Image Recognition]    
- 2013: [Deep Learning using Linear Support Vector Machines](https://arxiv.org/pdf/1306.0239) (Tang) [Computer Vision]    
- 2013: [Word2Vec introduces efficient word embeddings](https://arxiv.org/pdf/1301.3781) [Search Engines]    
- 2013: [Sequence-to-sequence models emerge](https://arxiv.org/abs/1409.3215) [Machine Translation]    
- 2014: [Attention mechanism introduced](https://arxiv.org/abs/1409.0473) [Neural Machine Translation]    
- 2015: [ResNet surpasses human-level performance on ImageNet](https://ieeexplore.ieee.org/document/7780459) [Image Classification]
    

### Pre-trained Language Models (PLMs) Era: 2018-2020

*PLMs introduced the "pre-training and fine-tuning" paradigm, training on large volumes of text before task-specific fine-tuning.*

- 2017: [Attention is All You Need](https://arxiv.org/abs/1706.03762) [Language Translation]    
- 2018: [ULMFiT](https://paperswithcode.com/method/ulmfit) (Universal Language Model Fine-tuning) [Text classification]    
- 2018: [ELMo](https://paperswithcode.com/method/elmo) (Embeddings from Language Models) [Named Entity Recognition]    
- 2018: [BERT](https://aclanthology.org/N19-1423.pdf) (Bidirectional Encoder Representations from Transformers) [Question answering]    
- 2019: [GPT-2](https://openai.com/index/better-language-models/) [Text completion and generation]    
- 2019: [XLNet](https://www.semanticscholar.org/paper/XLNet-Transfer-Learning-Model-for-Sentimental-Dhivyaa-Nithya/4b402bc52446f018f3fe7a859d0cd03027c91e5a) [Sentiment analysis]    
- 2019: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) [Natural language inference]    
- 2020: [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB) [Token classification tasks]
    
### Large Language Models (LLMs) Era: 2020-Present

*LLMs are trained on massive text corpora with billions of parameters, approximating human-level performance in various tasks.*

- 2020: [GPT-3](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) [OpenAI, 175B] [Few-shot learning across various NLP tasks]    
- 2020: [GShard](https://arxiv.org/abs/2006.16668) [Google, 600B] [Multilingual translation]    
- 2021: [Switch Transformer](https://arxiv.org/abs/2101.03961) [Google, 1.6T] [Efficient language modeling]    
- 2021: [Megatron-Turing NLG](https://arxiv.org/abs/2201.11990) [Microsoft & NVIDIA, 530B] [Natural language generation]    
- 2022: [PaLM](https://research.google/blog/pathways-language-model-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/) [Google, 540B] [Reasoning and problem-solving]    
- 2022: [BLOOM](https://arxiv.org/abs/2211.05100) [BigScience, 176B] [Open-source multilingual language model]    
- 2023: [GPT-4](https://arxiv.org/abs/2303.08774) [OpenAI, undisclosed] [Advanced language understanding and generation]
